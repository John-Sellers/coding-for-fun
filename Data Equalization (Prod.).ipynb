{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "146b7a2f-00d1-498c-99f3-6316ae9df32c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Calling in data from the catalog\n",
    "df = spark.read.table(\"main.flexgen_qa.assets_bms_unmanaged\")\n",
    "\n",
    "# Convert Unix timestamp to timestamp. Used the new timestamp column over time_record column\n",
    "# because the timestamp column is already truncated down to the second compared to time_record\n",
    "df = df.withColumn('time', (col('time').cast(DoubleType())/1000000))\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(from_unixtime(\"time\", \"yyyy-MM-dd HH:mm:ss\")))\n",
    "\n",
    "# Dropping unnecessary columns from dataframe as we do not want to take the mean/mode of them\n",
    "# _metadata: The filepath of the data from AWS S3\n",
    "# ingestion_time: The time the data was stored\n",
    "# time: Was initially in unix format and then converted to timestamp and stored in the timestamp column\n",
    "# _rescued_data: Has all null values, not exactly sure what that field is for\n",
    "# time_record: Is the same as time but down to the millesecond value, we prefer second value\n",
    "df = df.drop('_metadata', 'name', 'ingestion_time', 'time', '_rescued_data', 'time_record')\n",
    "\n",
    "def aggregate_data(df: DataFrame, groupby_cols: list, numeric_cols: list, non_numeric_cols: list) -> DataFrame:\n",
    "    '''\n",
    "    Description:\n",
    "    This function takes a DataFrame, a list of groupby columns, a list of numeric columns, and a list\n",
    "    of non-numeric columns as inputs and returns an aggregated DataFrame with one row per second,\n",
    "    aggregating both numeric and non-numeric columns while considering the groupby columns.\n",
    "\n",
    "    Inputs:\n",
    "    df (PySpark DataFrame): Input DataFrame.\n",
    "    groupby_cols (List): List of columns to group by, including 'timestamp'.\n",
    "    numeric_cols (List): List of numeric columns for aggregation.\n",
    "    non_numeric_cols (List): List of non-numeric columns for aggregation.\n",
    "\n",
    "    Returns:\n",
    "    A PySpark DataFrame with one row per second and aggregated values, considering the groupby columns.\n",
    "    '''\n",
    "\n",
    "    # Grouping by the specified columns and aggregating both numeric and non-numeric columns\n",
    "    aggregated_df = df.groupBy(*groupby_cols).agg(\n",
    "        *(median(col).alias(col) for col in numeric_cols),  # Numeric aggregation\n",
    "        *(mode(col).alias(col) for col in non_numeric_cols)  # Non-numeric aggregation\n",
    "    )\n",
    "\n",
    "    # Dropping duplicates because if they're not dropped they will cause the groupby columns to appear twice in the dataframe\n",
    "    aggregated_df = aggregated_df.dropDuplicates(groupby_cols)\n",
    "\n",
    "    # Sorting the resulting DataFrame by 'timestamp' to have the dataframe in ascending order\n",
    "    aggregated_df = aggregated_df.sort(col('timestamp'))\n",
    "\n",
    "    return aggregated_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Equalization (Prod.)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
