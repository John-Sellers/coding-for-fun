{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from South River historcal data\n",
    "df = pd.read_parquet(r'C:\\Users\\JohnSellers\\Downloads\\South_River_Mar_Aug.parquet')\n",
    "\n",
    "# Save to csv\n",
    "df.to_csv(r'C:\\Users\\JohnSellers\\Downloads\\South_River_Mar_Aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set time column as index\n",
    "df['time'] = pd.to_datetime(df['time']).dt.tz_convert('US/Eastern')\n",
    "\n",
    "## Build clean_df\n",
    "bms01_df = pd.DataFrame()\n",
    "\n",
    "bms01_df['time'] = df['time']\n",
    "\n",
    "## Site SOC is the average SOC of all ESS\n",
    "# clean_df['Site_SOC'] = soc_df.mean(axis = 1)\n",
    "bms01_df['ESS001_BMS_soc'] = df['ESS001_BMS01_soc']\n",
    "\n",
    "# Determing BMS # to dechipher between BMS01 and BMS02\n",
    "bms01_df['BMS'] = 'BMS01'\n",
    "\n",
    "\n",
    "## Add meters data\n",
    "bms01_df['Site_kwh_Delivered'] = df['sel_735_kwh_delivered']\n",
    "bms01_df['Site_kwh_Received'] = df['sel_735_kwh_received']\n",
    "bms01_df['Site_Active_Power'] = df['sel_735_active_power']\n",
    "\n",
    "## Calculate SOC difference to divide into individual charging/discharging events\n",
    "bms01_df['SOC_Diff'] = bms01_df.ESS001_BMS_soc.diff()\n",
    "bms01_df['Delivered_Diff'] = bms01_df.Site_kwh_Delivered.diff()\n",
    "bms01_df['Received_Diff'] = bms01_df.Site_kwh_Received.diff()\n",
    "\n",
    "# # Viewing the dataframe to ensure it is correct\n",
    "# bms01_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If diff is greater than 0, it's charging\n",
    "## If diff is less than 0, it is discharging \n",
    "discharging_timestamp = False\n",
    "charging_timestamp = False\n",
    "# nameplate_capacity = 21600\n",
    "# energy_per_percent = nameplate_capacity / 100\n",
    "\n",
    "recal_df = pd.DataFrame(columns = ['Start Time', 'Starting SOC','Starting Energy', 'End Time', 'Ending SOC', 'Ending Energy', 'Type', 'SOC Delta', 'Energy Delta', 'Energy per Percent SOC'])\n",
    "\n",
    "for index, row in bms01_df.iterrows():\n",
    "\n",
    "    # a positive difference indicates a possible charging period\n",
    "    if row['SOC_Diff'] > 0:\n",
    "        # if we do not already have a possible starting charging time, grab the values\n",
    "        if not charging_timestamp and not (row['ESS001_BMS_soc'] > 80):\n",
    "            charging_timestamp = row['time']\n",
    "            starting_soc = row['ESS001_BMS_soc']\n",
    "            starting_energy = row['Site_kwh_Received']\n",
    "        # if we have a possible starting discharge time, clear it\n",
    "        # if it was a true discharge, soc would not increase\n",
    "        elif discharging_timestamp:\n",
    "            discharging_timestamp = False\n",
    "\n",
    "    # a negative difference indicates a possible discharge period\n",
    "    elif row['SOC_Diff'] < 0 and not (row['ESS001_BMS_soc'] < 15):\n",
    "        # if we do not already have a possible starting discharging time, grab the values\n",
    "        if not discharging_timestamp:\n",
    "            discharging_timestamp = row['time']\n",
    "            starting_soc = row['ESS001_BMS_soc']\n",
    "            starting_energy = row['Site_kwh_Delivered']\n",
    "        # if we have a possible starting charging time, clear it\n",
    "        # if it was a true charge, soc would not decrease\n",
    "        elif charging_timestamp:\n",
    "            charging_timestamp = False\n",
    "\n",
    "    # when the difference is 0, we check to see if a charge or discharge has finished\n",
    "    elif row['SOC_Diff'] == 0:\n",
    "        # if we have a possible discharge, check to see if soc is below 15\n",
    "        if discharging_timestamp:\n",
    "            if row['ESS001_BMS_soc'] < 15:\n",
    "                # we have found a discharging event ! yay !\n",
    "                ending_soc = row['ESS001_BMS_soc']\n",
    "                ending_energy = row['Site_kwh_Delivered']\n",
    "                soc_delta = starting_soc - ending_soc\n",
    "                energy_delta = ending_energy - starting_energy\n",
    "                energy_per_soc = energy_delta / soc_delta\n",
    "                recal_df.loc[len(recal_df)] = [discharging_timestamp, starting_soc, starting_energy, row['time'], ending_soc, ending_energy, 'Discharge', soc_delta, energy_delta, energy_per_soc]\n",
    "                discharging_timestamp = False\n",
    "            else:\n",
    "                discharging_timestamp = False\n",
    "        # if we have a possible charge, check to see if soc is above 95\n",
    "        elif charging_timestamp:\n",
    "            if row['ESS001_BMS_soc'] > 95:\n",
    "                # we have found a charging event ! yay !\n",
    "                ending_soc = row['ESS001_BMS_soc']\n",
    "                ending_energy = row['Site_kwh_Received']\n",
    "                soc_delta = ending_soc - starting_soc\n",
    "                energy_delta = ending_energy - starting_energy\n",
    "                energy_per_soc = energy_delta / soc_delta\n",
    "                recal_df.loc[len(recal_df)] = [charging_timestamp, starting_soc, starting_energy, row['time'], ending_soc, ending_energy, 'Charge', soc_delta, energy_delta, energy_per_soc]\n",
    "                charging_timestamp = False\n",
    "            else:\n",
    "                charging_timestamp = False\n",
    "\n",
    "# # Viewing new dataframe to see if it looks correct\n",
    "# recal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all observations that fall betwwen recal_df['Start Time'] and recal_df['End Time'] in bms01_df and creating \n",
    "# a new dataframe with these observations. This dataframe will be used for training, validating and testing the model\n",
    "df = pd.DataFrame()\n",
    "for i in range(len(recal_df)):\n",
    "    df = pd.concat([df, bms01_df[(bms01_df['time'] >= recal_df['Start Time'][i]) & (bms01_df['time'] <= recal_df['End Time'][i])]])\n",
    "\n",
    "obs_df = df.reset_index(drop=True)\n",
    "# obs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determind when the observation is charging, discharging, or idle so that we can break down the data into charge, discharge, and idle dataframes\n",
    "obs_df['Type'] = np.where(obs_df['ESS001_BMS_soc'].shift(-1) > obs_df['ESS001_BMS_soc'], 'Charge', 'Discharge')\n",
    "# obs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking down the obs_df into charge and discharge cycles and saving them as separate charge_df and discharge_df\n",
    "charge_df = obs_df[obs_df['Type'] == 'Charge']\n",
    "discharge_df = obs_df[obs_df['Type'] == 'Discharge']\n",
    "\n",
    "# Checking the unique values in the 'Type' column to ensure that the above operation was successful\n",
    "# charge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discharge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to create a basic linear regression model\n",
    "# This function will be used to create a linear regression model trained on the training data\n",
    "# The model will then be used to predict the target variable for the validation and testing data\n",
    "# The function will also return the mean squared error and r-squared values for the validation and testing data\n",
    "\n",
    "def linear_regression_model(X, y, train_size, test_size, seed):\n",
    "    # Splitting the whole dataset into training and remainder sets\n",
    "    # The remainder with be split into validation and test sets\n",
    "    X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=train_size, random_state=seed)\n",
    "\n",
    "    # Splitting the remainder set into validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=test_size, random_state=seed)\n",
    "\n",
    "    # # Printing the shapes of the training, validation, and test sets to ensure that the proportions are correct\n",
    "    # print(X_train.shape, y_train.shape)\n",
    "    # print(X_val.shape, y_val.shape)\n",
    "    # print(X_test.shape, y_test.shape)\n",
    "\n",
    "    # Creating a linear regression object\n",
    "    lin_reg = LinearRegression()\n",
    "    \n",
    "    # Training the linear regression model on the training data\n",
    "    pred = lin_reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the target variable for all three datasets\n",
    "    y_train_pred = pred.predict(X_train)\n",
    "    y_val_pred = pred.predict(X_val)\n",
    "    y_test_pred = pred.predict(X_test)\n",
    "\n",
    "    # Storing predicted and actual values in a dataframe to be viewd later and compared\n",
    "    predicted_train_df = pd.DataFrame({\"predicted\":y_train_pred, \"actual\": y_train}).reset_index(drop=True)\n",
    "    predicted_val_df = pd.DataFrame({\"predicted\":y_val_pred, \"actual\": y_val}).reset_index(drop=True)\n",
    "    predicted_test_df = pd.DataFrame({\"predicted\":y_test_pred, \"actual\": y_test}).reset_index(drop=True)\n",
    "\n",
    "    # Viewing the predicted values vs the actual values of the linear regression model to see how well it fits the data\n",
    "    with pd.option_context('plotting.backend', \"plotly\"):\n",
    "        fig = predicted_train_df.head(1000).plot().update_layout(title = \"Predicted vs Actual Values for Training Data\", xaxis_title=\"Features\", yaxis_title=\"BMS SOC\")\n",
    "        fig.show()\n",
    "\n",
    "    with pd.option_context('plotting.backend', \"plotly\"):\n",
    "        fig = predicted_val_df.head(1000).plot().update_layout(title = \"Predicted vs Actual Values for Validation Data\", xaxis_title=\"Features\", yaxis_title=\"BMS SOC\")\n",
    "        fig.show()\n",
    "\n",
    "    with pd.option_context('plotting.backend', \"plotly\"):\n",
    "        fig = predicted_test_df.head(1000).plot().update_layout(title = \"Predicted vs Actual Values for Test Data\", xaxis_title=\"Features\", yaxis_title=\"BMS SOC\")\n",
    "        fig.show()\n",
    "    \n",
    "    # Calculating the mean squared error for all three datasets\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Calculating the r-squared value for all three datasets\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Train MSE':train_mse, \n",
    "        'Train R2':train_r2, \n",
    "        'Val MSE':val_mse, \n",
    "        'Val R2':val_r2, \n",
    "        'Test MSE':test_mse, \n",
    "        'Test R2':test_r2},\n",
    "        index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to create a basic random forest model\n",
    "# This function will be used to create a random forest regression model trained on the training data\n",
    "# The model will then be used to predict the target variable for the validation and testing data\n",
    "# The function will also return the mean squared error and r-squared values for the validation and testing data\n",
    "\n",
    "def random_forest_model(X, y, train_size, test_size, seed, n_estimators, max_depth, max_leaf_nodes):\n",
    "    # Splitting the whole dataset into training and remainder sets\n",
    "    # The remainder with be split into validation and test sets\n",
    "    X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=train_size, random_state=seed)\n",
    "\n",
    "    # Splitting the remainder set into validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=test_size, random_state=seed)\n",
    "\n",
    "    # Printing the shapes of the training, validation, and test sets to ensure that the proportions are correct\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_val.shape, y_val.shape)\n",
    "    print(X_test.shape, y_test.shape)\n",
    "\n",
    "    # Creating a linear regression object\n",
    "    rf_reg = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_leaf_nodes=max_leaf_nodes)\n",
    "\n",
    "    # Training the linear regression model on the training data\n",
    "    pred = rf_reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the target variable for all three datasets\n",
    "    y_train_pred = pred.predict(X_train)\n",
    "    y_val_pred = pred.predict(X_val)\n",
    "    y_test_pred = pred.predict(X_test)\n",
    "\n",
    "    # Storing predicted and actual values in a dataframe to be viewd later and compared\n",
    "    predicted_train_df = pd.DataFrame({\"predicted\":y_train_pred, \"actual\": y_train}).reset_index(drop=True)\n",
    "    predicted_val_df = pd.DataFrame({\"predicted\":y_val_pred, \"actual\": y_val}).reset_index(drop=True)\n",
    "    predicted_test_df = pd.DataFrame({\"predicted\":y_test_pred, \"actual\": y_test}).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Viewing the predicted values vs the actual values of the linear regression model to see how well it fits the data\n",
    "    with pd.option_context('plotting.backend', \"plotly\"):\n",
    "        fig = predicted_train_df.head(1000).plot().update_layout(title = \"Predicted vs Actual Values for Training Data\", xaxis_title=\"Features\", yaxis_title=\"BMS SOC\")\n",
    "        fig.show()\n",
    "\n",
    "    with pd.option_context('plotting.backend', \"plotly\"):\n",
    "        fig = predicted_val_df.head(1000).plot().update_layout(title = \"Predicted vs Actual Values for Validation Data\", xaxis_title=\"Features\", yaxis_title=\"BMS SOC\")\n",
    "        fig.show()\n",
    "\n",
    "    with pd.option_context('plotting.backend', \"plotly\"):\n",
    "        fig = predicted_test_df.head(1000).plot().update_layout(title = \"Predicted vs Actual Values for Test Data\", xaxis_title=\"Features\", yaxis_title=\"BMS SOC\")\n",
    "        fig.show()\n",
    "    \n",
    "    # Calculating the mean squared error for all three datasets\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Calculating the r-squared value for all three datasets\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Returning the mean squared error and r-squared values for all three datasets\n",
    "    return pd.DataFrame({\n",
    "        'Train MSE':train_mse, \n",
    "        'Train R2':train_r2, \n",
    "        'Val MSE':val_mse, \n",
    "        'Val R2':val_r2, \n",
    "        'Test MSE':test_mse, \n",
    "        'Test R2':test_r2},\n",
    "        index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to create a basic random forest model\n",
    "# This function will be used to create a random forest regression model trained on the training data\n",
    "# The model will then be used to predict the target variable for the validation and testing data\n",
    "# The function will also return the mean squared error and r-squared values for the validation and testing data\n",
    "\n",
    "def xgboost_reg_model(X, y, train_size, test_size, seed, n_estimators, learning_rate, max_depth): #, n_estimators, max_depth, max_leaves\n",
    "    # Splitting the whole dataset into training and remainder sets\n",
    "    # The remainder with be split into validation and test sets\n",
    "    X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=train_size, random_state=seed)\n",
    "\n",
    "    # Splitting the remainder set into validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=test_size, random_state=seed)\n",
    "\n",
    "    # Printing the shapes of the training, validation, and test sets to ensure that the proportions are correct\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_val.shape, y_val.shape)\n",
    "    print(X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "    # Creating a linear regression object\n",
    "    xgb_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth) #n_estimators, max_depth, max_leaves\n",
    "\n",
    "    # Training the linear regression model on the training data\n",
    "    pred = xgb_reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the target variable for all three datasets\n",
    "    y_train_pred = pred.predict(X_train)\n",
    "    y_val_pred = pred.predict(X_val)\n",
    "    y_test_pred = pred.predict(X_test)\n",
    "\n",
    "    # Storing predicted and actual values in a dataframe to be viewd later and compared\n",
    "    predicted_train_df = pd.DataFrame({\"predicted\":y_train_pred, \"actual\": y_train}).reset_index(drop=True)\n",
    "    predicted_val_df = pd.DataFrame({\"predicted\":y_val_pred, \"actual\": y_val}).reset_index(drop=True)\n",
    "    predicted_test_df = pd.DataFrame({\"predicted\":y_test_pred, \"actual\": y_test}).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Viewing the predicted values vs the actual values of the linear regression model to see how well it fits the data\n",
    "    with pd.option_context('plotting.backend', \"plotly\"):\n",
    "        fig = predicted_train_df.head(1000).plot().update_layout(title = \"Predicted vs Actual Values for Training Data\", xaxis_title=\"Features\", yaxis_title=\"BMS SOC\")\n",
    "        fig.show()\n",
    "\n",
    "    with pd.option_context('plotting.backend', \"plotly\"):\n",
    "        fig = predicted_val_df.head(1000).plot().update_layout(title = \"Predicted vs Actual Values for Validation Data\", xaxis_title=\"Features\", yaxis_title=\"BMS SOC\")\n",
    "        fig.show()\n",
    "\n",
    "    with pd.option_context('plotting.backend', \"plotly\"):\n",
    "        fig = predicted_test_df.head(1000).plot().update_layout(title = \"Predicted vs Actual Values for Test Data\", xaxis_title=\"Features\", yaxis_title=\"BMS SOC\")\n",
    "        fig.show()\n",
    "    \n",
    "    # Calculating the mean squared error for all three datasets\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Calculating the r-squared value for all three datasets\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Returning the mean squared error and r-squared values for all three datasets\n",
    "    return pd.DataFrame({\n",
    "        'Train MSE':train_mse, \n",
    "        'Train R2':train_r2, \n",
    "        'Val MSE':val_mse, \n",
    "        'Val R2':val_r2, \n",
    "        'Test MSE':test_mse, \n",
    "        'Test R2':test_r2},\n",
    "        index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charge/Discharge Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_lr = linear_regression_model(\n",
    "    obs_df[['Site_kwh_Delivered', 'Site_kwh_Received', 'Received_Diff']], \n",
    "    obs_df['ESS001_BMS_soc'], \n",
    "    train_size=0.75, \n",
    "    test_size=0.5,\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_rf = random_forest_model(\n",
    "    obs_df[['Site_kwh_Delivered','Site_kwh_Received', 'Received_Diff']], \n",
    "    obs_df['ESS001_BMS_soc'], \n",
    "    train_size=0.80, \n",
    "    test_size=0.5, \n",
    "    seed=0, \n",
    "    n_estimators=200, \n",
    "    max_depth=15,\n",
    "    max_leaf_nodes=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_xgb = xgboost_reg_model(\n",
    "    obs_df[['Site_kwh_Delivered','Site_kwh_Received', 'Received_Diff']], \n",
    "    obs_df['ESS001_BMS_soc'], \n",
    "    train_size=0.80, \n",
    "    test_size=0.5, \n",
    "    seed=0,\n",
    "    n_estimators=200,\n",
    "    learning_rate=.1,\n",
    "    max_depth=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charging Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_lr = linear_regression_model(\n",
    "    charge_df[['Site_kwh_Delivered', 'Site_kwh_Received', 'Received_Diff']], \n",
    "    charge_df['ESS001_BMS_soc'], \n",
    "    train_size=0.75, \n",
    "    test_size=0.5, \n",
    "    seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_rf = random_forest_model(\n",
    "    charge_df[['Site_kwh_Delivered','Site_kwh_Received', 'Received_Diff']], \n",
    "    charge_df['ESS001_BMS_soc'], \n",
    "    train_size=0.80, \n",
    "    test_size=0.5, \n",
    "    seed=0, \n",
    "    n_estimators=200, \n",
    "    max_depth=15,\n",
    "    max_leaf_nodes=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_xgb = xgboost_reg_model(\n",
    "    charge_df[['Site_kwh_Delivered','Site_kwh_Received', 'Received_Diff']], \n",
    "    charge_df['ESS001_BMS_soc'], \n",
    "    train_size=0.80, \n",
    "    test_size=0.5, \n",
    "    seed=0,\n",
    "    n_estimators=200,\n",
    "    learning_rate=.1,\n",
    "    max_depth=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discharging Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_lr = linear_regression_model(\n",
    "    discharge_df[['Site_kwh_Delivered', 'Site_kwh_Received', 'Received_Diff']], \n",
    "    discharge_df['ESS001_BMS_soc'], \n",
    "    train_size=0.75, \n",
    "    test_size=0.5, \n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_rf = random_forest_model(\n",
    "    discharge_df[['Site_kwh_Delivered', 'Site_kwh_Received', 'Received_Diff']], \n",
    "    discharge_df['ESS001_BMS_soc'], \n",
    "    train_size=0.75, \n",
    "    test_size=0.5, \n",
    "    seed=0, \n",
    "    n_estimators=200, \n",
    "    max_depth=15, \n",
    "    max_leaf_nodes=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_xgb = xgboost_reg_model(\n",
    "    discharge_df[['Site_kwh_Delivered','Site_kwh_Received', 'Received_Diff']], \n",
    "    discharge_df['ESS001_BMS_soc'], \n",
    "    train_size=0.80, \n",
    "    test_size=0.5, \n",
    "    seed=0,\n",
    "    n_estimators=200,\n",
    "    learning_rate=.1,\n",
    "    max_depth=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.concat([cd_lr, cd_rf, cd_xgb, c_lr, c_rf, c_xgb, d_lr, d_rf, d_xgb], axis=0).reset_index(drop=True)\n",
    "metrics_df.index = ['CD Linear Regression', 'CD Random Forest', 'CD XGBoost', 'C Linear Regression', 'C Random Forest', 'C XGBoost', 'D Linear Regression', 'D Random Forest', 'D XGBoost']\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
