{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8DzFwydHNqp"
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Configuration\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boMQuk8MT7mm"
   },
   "source": [
    "## **Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "JXTV5sXxHpM_",
    "outputId": "0cc8ef2c-3451-4427-820e-9658ed644d9c"
   },
   "outputs": [],
   "source": [
    "# Data ingestion\n",
    "data = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\n",
    "df = pd.read_csv(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nN0iq4PGIDC5",
    "outputId": "87757f62-5124-4780-e50e-87bc555e92d3"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"{df.description.nunique() = }\",\n",
    "    f\"\\n{df.tag.nunique() = }\",\n",
    "    f\"\\n{df.tag.value_counts()}\",\n",
    "    f\"\\n{df.created_on.nunique() = }\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvVUu9kPIn4i"
   },
   "outputs": [],
   "source": [
    "# Splitting data into training\n",
    "\n",
    "## Setting aside 20% of data to later be split between the test and validation\n",
    "## sets\n",
    "rem_size = 0.2\n",
    "test_size = 0.5\n",
    "\n",
    "train_df, rem_df = train_test_split(df, test_size=rem_size, random_state=0, stratify=df.tag)\n",
    "\n",
    "\n",
    "# Splitting the rem_df to create our test and validation sets\n",
    "val_df, test_df = train_test_split(rem_df, test_size=test_size, random_state=0, stratify=rem_df.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBTQ3IsFLnF_",
    "outputId": "74cafd39-1ebf-4faf-f3c9-ab6f7a5e5dd8"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Shape of train_df {train_df.shape}\"\n",
    "    f\"\\nShape of val_df {val_df.shape}\"\n",
    "    f\"\\nShape of test_df {test_df.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyGva4cLOExY",
    "outputId": "66148fb1-a553-4ff3-f5a8-da404ac39488"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Count of 'Tags':\"\n",
    "    f\"\\n{df.tag.value_counts()}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igYLWRcYOmZt"
   },
   "outputs": [],
   "source": [
    "# Get the tag counts\n",
    "tag_counts = df.tag.value_counts()\n",
    "\n",
    "# Separate values and tags\n",
    "tags = tag_counts.index\n",
    "tag_values = tag_counts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "CU869ErGO9gZ",
    "outputId": "b2743961-aa0e-46f0-fe85-8083604ac8b5"
   },
   "outputs": [],
   "source": [
    "# Plot tag frequencies\n",
    "plt.figure(figsize=(10, 3))\n",
    "ax = sns.barplot(x=list(tags), y=list(tag_values))\n",
    "ax.set_xticklabels(tags, rotation=0, fontsize=8)\n",
    "plt.title(\"Tag distribution\", fontsize=14)\n",
    "plt.ylabel(\"# of projects\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "wKasaYO1P4g7",
    "outputId": "9fa9c6ef-95c5-434d-f649-a312dda3535c"
   },
   "outputs": [],
   "source": [
    "# Most frequent tokens for each tag\n",
    "tag=\"natural-language-processing\"\n",
    "plt.figure(figsize=(10, 3))\n",
    "subset = df[df.tag==tag]\n",
    "text = subset.title.values\n",
    "cloud = WordCloud(\n",
    "    stopwords=STOPWORDS, background_color=\"black\", collocations=False,\n",
    "    width=500, height=300).generate(\" \".join(text))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aD18lNwTTKZ"
   },
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "rvcaYV8AS7x4",
    "outputId": "fe5655d6-1dcb-4c72-da20-8ad7fc5451b6"
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "df['text'] = df.title + \" \" + df.description\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1r2swCX1U5nj"
   },
   "outputs": [],
   "source": [
    "def clean_text(text, stopwords=STOPWORDS):\n",
    "    \"\"\"Clean raw text string.\"\"\"\n",
    "    # Lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stopwords\n",
    "    pattern = re.compile(r'\\b(' + r\"|\".join(stopwords) + r\")\\b\\s*\")\n",
    "    text = pattern.sub('', text)\n",
    "\n",
    "    # Spacing and filters\n",
    "    text = re.sub(r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)  # add spacing\n",
    "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)  # remove non alphanumeric chars\n",
    "    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n",
    "    text = text.strip()  # strip white space at the ends\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  #  remove links\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGlW4cXqU_2V"
   },
   "outputs": [],
   "source": [
    "# Apply clean_text function to dataframe\n",
    "original_df = df.copy()\n",
    "df.text = df.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_NXuiNtfcL8Q",
    "outputId": "5931896b-d081-4923-92da-bbd3d9bab6a4"
   },
   "outputs": [],
   "source": [
    "print (f\"{original_df.text.values[2]}\\n{df.text.values[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame cleanup\n",
    "cleaned_df = df[[\"text\", \"tag\"]]\n",
    "cleaned_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.dropna(subset=[\"tag\"])\n",
    "cleaned_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assiging LabelEncoder to le variable to later be used\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and transform the 'tag' column\n",
    "cleaned_df['tag'] = le.fit_transform(cleaned_df['tag'])\n",
    "\n",
    "cleaned_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping unique values to their encoded numbers\n",
    "label_encoder_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "# Print the mapping of unique values to their encoded numbers\n",
    "print(\"Label Encoding Mapping:\")\n",
    "for tag, encoded_number in label_encoder_mapping.items():\n",
    "    print(f\"{tag}: {encoded_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used to revert label encoded values back to their original text\n",
    "# that will allow for users to understand the output\n",
    "def decode_labels(df: pd.DataFrame, column_name: str, label_encoder: LabelEncoder) -> pd.DataFrame:\n",
    "    df[column_name + '_decoded'] = label_encoder.inverse_transform(df[column_name])\n",
    "    return df\n",
    "\n",
    "# Decode label-encoded values\n",
    "df_decoded = decode_labels(cleaned_df, 'tag', le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert tokenizer\n",
    "# This line initializes a BERT tokenizer from the Hugging Face library.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "\n",
    "# The input text to be tokenized\n",
    "text = \"Transfer learning with transformers for text classification.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "# This line tokenizes the input text using the BERT tokenizer.\n",
    "# It returns a dictionary of encoded inputs, including the input IDs and attention mask.\n",
    "# The input text is passed as a list to handle batching, and \"np\" indicates that the return type should be numpy arrays.\n",
    "# \"padding\" parameter is set to \"longest\" to pad sequences to the length of the longest sequence in the batch.\n",
    "encoded_inputs = tokenizer([text], return_tensors=\"np\", padding=\"longest\")\n",
    "\n",
    "# Print input IDs\n",
    "print (\"input_ids:\", encoded_inputs[\"input_ids\"])\n",
    "\n",
    "# Print attention mask\n",
    "print (\"attention_mask:\", encoded_inputs[\"attention_mask\"])\n",
    "\n",
    "# Decode the input IDs\n",
    "print (tokenizer.decode(encoded_inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "    encoded_inputs = tokenizer(batch[\"text\"].tolist(), return_tensors=\"np\", padding=\"longest\")\n",
    "    return dict(ids=encoded_inputs[\"input_ids\"], masks=encoded_inputs[\"attention_mask\"], targets=np.array(batch[\"tag\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenize(cleaned_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @article{madewithml,\n",
    "#     author       = {Goku Mohandas},\n",
    "#     title        = { Preprocessing - Made With ML },\n",
    "#     howpublished = {\\url{https://madewithml.com/}},\n",
    "#     year         = {2023}\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "boMQuk8MT7mm"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
